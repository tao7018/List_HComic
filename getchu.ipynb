{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "みちきんぐ 1 num\n",
      "========v1\n",
      "page:1\n",
      "1 .\n",
      "ok\n",
      "3 .. 2 .. 1 ..\n",
      "Press Any Key To Exit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "import urllib , requests , sys ,string ,time\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "from time import gmtime, strftime\n",
    "\n",
    "#getchu\n",
    "#fkey作者來源key.txt\n",
    "#fcomiclist結果key_getchu.text\n",
    "\n",
    "#輸出格式：\n",
    "#getchu\n",
    "#!作者\n",
    "#!總筆數\n",
    "#==類別_數量_中文敘述\n",
    "#類別_品名\n",
    "#網址\n",
    "\n",
    "#頁顯示數量\n",
    "pnum = 30\n",
    "mlink = 'http://www.getchu.com/'\n",
    "\n",
    "fkey = open('key.txt', 'r')\n",
    "key = fkey.readline()#key=作者\n",
    "fkey.close()\n",
    "\n",
    "#key=作者\n",
    "key2 = urllib.quote(key.decode('utf8').encode('euc_jp'))\n",
    "key3 = urllib.unquote(key2.decode('euc_jp').encode('utf8'))\n",
    "#網址用\n",
    "key4 = urllib.quote(key)\n",
    "#print key4, '\\n' , urllib.quote(key.decode('sjis').encode('utf8')) \n",
    "\n",
    "#檢查BOM\n",
    "if '%EF%BB%BF' in urllib.quote(key):\n",
    "    print 'fuck ms'\n",
    "\n",
    "'''網址樣本\n",
    "http://www.getchu.com/php/search.phtml?search_keyword=&search_title=&search_brand=&\n",
    "search_person=%BE%E5%C6%A3%C0%AF%BC%F9\n",
    "&search_jan=&search_isbn=&genre=all&start_date=&end_date=&age=&list_count=30&sort=sales&sort2=down&list_type=list&search=1&\n",
    "pageID=1\n",
    "'''\n",
    "link = \"http://www.getchu.com/php/search.phtml?search_keyword=&search_title=&search_brand=&search_person=\" + urllib.quote(key3) + \"&search_jan=&search_isbn=&genre=all&start_date=&end_date=&age=&list_count=30&sort=sales&sort2=down&list_type=list&search=1&pageID=1\"\n",
    "\n",
    "#head = {'User-agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36'}\n",
    "res = requests.get(link)#, headers = head)\n",
    "\n",
    "res.encoding =  res.apparent_encoding#亂碼處理\n",
    "soup = BeautifulSoup(res.text,\"html.parser\")#超出lxml緩存，改其他存取\n",
    "\n",
    "def next(page = 2):\n",
    "    link = \"http://www.getchu.com/php/search.phtml?search_keyword=&search_title=&search_brand=&search_person=\" + urllib.quote(key3) + \"&search_jan=&search_isbn=&genre=all&start_date=&end_date=&age=&list_count=30&sort=sales&sort2=down&list_type=list&search=1&pageID=\"+ str(page)\n",
    "    \n",
    "    res = requests.get(link)\n",
    "    res.encoding =  res.apparent_encoding\n",
    "    \n",
    "    only_a_tags = SoupStrainer(\"ul\", class_=\"display\")#縮小處理範圍\n",
    "    #先html.parser解析與縮小範圍，再以字串給lxml\n",
    "    #soup = BeautifulSoup(str(BeautifulSoup(res.text,\"html.parser\",  parse_only=only_a_tags)),\"lxml\")\n",
    "    soup = BeautifulSoup(res.text,\"html.parser\",  parse_only=only_a_tags)\n",
    "    \n",
    "    return soup\n",
    "\n",
    "#全轉半\n",
    "def Q2B(ustring):\n",
    "    fs=u'０１２３４５６７８９ＱｑＷｗＥｅＲｒＴｔＹｙＵｕＩｉＯｏＰｐＡａＳｓＤｄＦｆＧｇＨｈＪｊＫｋＬｌＺｚＸｘＣｃＶｖＢｂＮｎＭｍ'\n",
    "    hs=u'0123456789QqWwEeRrTtYyUuIiOoPpAaSsDdFfGgHhJjKkLlZzXxCcVvBbNnMm'\n",
    "    rstr = ''\n",
    "    for tm in ustring:\n",
    "        if fs.find(tm)+1:\n",
    "            tm = hs[fs.find(tm)]\n",
    "        rstr = rstr + tm\n",
    "    ustring = rstr\n",
    "    ustring=ustring.lower()\n",
    "    return ustring\n",
    "#半轉全\n",
    "def B2Q(ustring):\n",
    "    fs=u'０１２３４５６７８９ＱｑＷｗＥｅＲｒＴｔＹｙＵｕＩｉＯｏＰｐＡａＳｓＤｄＦｆＧｇＨｈＪｊＫｋＬｌＺｚＸｘＣｃＶｖＢｂＮｎＭｍ'\n",
    "    hs=u'0123456789QqWwEeRrTtYyUuIiOoPpAaSsDdFfGgHhJjKkLlZzXxCcVvBbNnMm'\n",
    "    rstr = ''\n",
    "    ustring=ustring.lower()\n",
    "    for tm in ustring:\n",
    "        if hs.find(tm)+1:\n",
    "            tm = fs[hs.find(tm)]\n",
    "        rstr = rstr + tm\n",
    "    ustring = rstr\n",
    "    return ustring\n",
    "\n",
    "#資料儲存\n",
    "def save(sdict , check=0):\n",
    "    #print listdata\n",
    "    for temp in listdata:\n",
    "        if sdict.get(temp):\n",
    "            #fout.write(temp.encode('utf8') +sdict[temp].encode('utf8')  + '\\n')\n",
    "            fout.write(sdict[temp].encode('utf8')  + '\\n')\n",
    "    #return\n",
    "\n",
    "#資料處理\n",
    "def findbook(soup , page = 1):\n",
    "    a =0\n",
    "    check = 0\n",
    "    for li in soup.select('li'):\n",
    "        sou = BeautifulSoup(str(li),\"lxml\")\n",
    "        ctype = sou.select('.orangeb')[0].text\n",
    "        cbook = sou.select('.blueb')[0].text\n",
    "        cdata = sou.select('.orangeb')[0].next_sibling.next_sibling[5:]\n",
    "        #print ctype,cbook,cdata\n",
    "        \n",
    "        #網址\n",
    "        blink = ''\n",
    "        blink = sou.select('.blueb')[0].get('href')\n",
    "        blink = mlink + blink[3:]\n",
    "        #print blink\n",
    "        \n",
    "        #類型\n",
    "        if u'・' in ctype:\n",
    "            stype = ctype.split(u'・')\n",
    "            dtype = stype[0] + ']'\n",
    "        else:\n",
    "            dtype = ctype\n",
    "        \n",
    "        #品名\n",
    "        book = cbook\n",
    "        \n",
    "        #日期處理\n",
    "        #print cdata\n",
    "        cdata = cdata.rstrip()\n",
    "        if len(cdata) < 8:#無日期\n",
    "            cdata = '0000/00/00'#填入日期\n",
    "            check = 4#新作\n",
    "        data = cdata\n",
    "        while listdata.count(data):#重複日期判斷\n",
    "            data = data[:8] + str(int(data[8:]) + 1).rjust(2,'0')#日期+1_十位數填0\n",
    "        listdata.append(data)\n",
    "            \n",
    "        '''\n",
    "        typea = ['[BOOKS]','[雑誌]','[同人]']\n",
    "        typeb = ['[アニメ]']\n",
    "        typec = ['[PCゲーム]','[DVDPG]']\n",
    "        #其他\n",
    "        グッズ\n",
    "        アダルトグッズ\n",
    "        音楽CD\n",
    "        グラビア\n",
    "        実写\n",
    "        #寫做DVD-PG，getchu用DVDPG\n",
    "        '''\n",
    "        \n",
    "        #寫入dict\n",
    "        book = dtype + '_' + book + '_' + '\\n!' + blink#類形+書名+網址\n",
    "        if check == 4:\n",
    "            dict4.setdefault(data,book)\n",
    "        elif dtype in [u'[BOOKS]',u'[雑誌]',u'[同人]'] :\n",
    "            if (u'ノベルズ' in book):\n",
    "                book= book[:book.find(u']')]+u'_ノベルズ]'+book[book.find(u']'):]\n",
    "            elif (u'文庫' in book):\n",
    "                book= book[:book.find(u']')]+u'_文庫]'+book[book.find(u']'):]\n",
    "            dict1.setdefault(data,book)\n",
    "        elif dtype in[u'[アニメ]']:\n",
    "            dict2.setdefault(data,book)\n",
    "        elif dtype in [u'[PCゲーム]',u'[DVDPG]']:\n",
    "            dict3.setdefault(data,book)\n",
    "        else:\n",
    "            dict5.setdefault(data,book)\n",
    "        \n",
    "        a = a + 1\n",
    "        print '\\r',a,\n",
    "    print '.'\n",
    "    #print '========'\n",
    "    #return\n",
    "\n",
    "########\n",
    "\n",
    "key=key.lower()\n",
    "pn = soup.select('.s_condition')[0].select('b')[0].text#資料筆數\n",
    "#資料筆數_是否數字\n",
    "if pn.isdigit():\n",
    "    #print soup.find_all('b')\n",
    "    if int(pn) > pnum:\n",
    "        print 'BIG'\n",
    "    \n",
    "    fout = open('output/'+key.decode('utf8') + '_getchuv1.txt', 'w')#寫入模式開檔\n",
    "    fout.write('getchu\\n')#getchu\n",
    "    print key , pn , 'num\\n========v1'\n",
    "    time.sleep(1)\n",
    "    fout.write('!' + key + '\\n!總筆數' + pn.encode('utf8') +'_'+ strftime(\"%Y/%m/%d,%H:%M\")+'->')\n",
    "    \n",
    "    p = 0#頁\n",
    "    #建空輸出用字典與陣列\n",
    "    dict1={}#BOOKS,雑誌,同人\n",
    "    dict2={}#アニメ\n",
    "    dict3={}#PCゲーム,DVD-PG\n",
    "    dict4={}#新\n",
    "    dict5={}#其他\n",
    "    listdata = []\n",
    "    \n",
    "    #資料處理\n",
    "    while (int(pn) - p * pnum) > 0:\n",
    "        p = p + 1\n",
    "        print 'page:' + str(p)\n",
    "        soup = next(p)#頁\n",
    "        \n",
    "        findbook(soup)#資料處理\n",
    "        time.sleep(1)\n",
    "    #print 'ook\\n',listdata\n",
    "    \n",
    "    #日期排序\n",
    "    listdata.sort()\n",
    "    \n",
    "    fout.write(strftime(\"%H:%M\")+'\\n')\n",
    "    temp = ''\n",
    "    #dict1_BOOKS,雑誌,同人輸出\n",
    "    fout.write('==book_' + str(len(dict1)) +'_BOOKS,雑誌,同人\\n')\n",
    "    save(dict1)\n",
    "    #dict2_アニメ輸出\n",
    "    fout.write('==anime_' + str(len(dict2)) +'_アニメ\\n')\n",
    "    save(dict2)\n",
    "    #dict3_PCゲーム,DVD-PG輸出\n",
    "    fout.write('==game_' + str(len(dict3)) +'_PCゲーム,DVD-PG\\n')\n",
    "    save(dict3)\n",
    "    #dict4_新輸出\n",
    "    fout.write('==new_' + str(len(dict4)) +'_新\\n')\n",
    "    save(dict4)\n",
    "    #dict5_其他輸出\n",
    "    fout.write('==other_' + str(len(dict5)) +'_其他\\n')\n",
    "    save(dict5)\n",
    "    \n",
    "    #sys.exit()################\n",
    "    fout.close()\n",
    "    print 'ok'\n",
    "elif pn:\n",
    "    print 'No Date'\n",
    "\n",
    "#結束讀秒\n",
    "x=3\n",
    "while x!=0:\n",
    "    print x,'..',\n",
    "    x=x-1\n",
    "    time.sleep(1)\n",
    "raw_input(\"\\nPress Any Key To Exit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kyo1\n",
      "����ͳ��\n",
      "kyo1\n",
      "kyo1\n",
      "kyo1\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "import urllib , requests , sys ,string ,time\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "\n",
    "k = '%BE%AE%CE%D3%CD%B3%B9%E2'\n",
    "fkey = open('key.txt', 'r')\n",
    "key = fkey.readline()#key=作者\n",
    "fkey.close()\n",
    "\n",
    "#key=作者\n",
    "key2 = urllib.quote(key.decode('utf8').encode('euc_jp'))\n",
    "print key2\n",
    "#key2 = urllib.quote(key.decode('sjis').encode('utf8'))\n",
    "key3 = urllib.unquote(k.decode('euc_jp').encode('utf8'))\n",
    "print key3\n",
    "\n",
    "print urllib.quote(key.decode('utf8').encode('iso2022_jp_1'))\n",
    "print urllib.quote(key.decode('utf8').encode('euc_jp'))\n",
    "#網址用\n",
    "key4 = urllib.quote(key)\n",
    "#print key4, '\\n' , urllib.quote(key.decode('sjis').encode('utf8')) \n",
    "print key4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012/09/28\n",
      "../soft.phtml?id=748187 1614\n",
      "http://www.getchu.com/soft.phtml?id=748187\n",
      "012aaa3456789\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "import urllib , requests , sys ,string ,time\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "\n",
    "html_doc ='''\n",
    "<li>\n",
    "<div class=\"content_block\">\n",
    "\n",
    "<div id=\"package_block\">\n",
    "<div class=\"package\">\n",
    "<A HREF=\"../soft.phtml?id=748187\" ><IMG class=\"lazy\" src=\"/common/images/space.gif\" data-original=\"http://www.getchu.com/brandnew/748187/c748187package_ss.jpg\" width=\"120\" height=\"173\" border=0></A><br>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div id=\"cart_block\">\n",
    "<div class=\"cart_block\">\n",
    "\n",
    "                                    <BR><!--CART-->\n",
    "<!--予約-->\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div id=\"detail_block\">\n",
    "<div class=\"content_block\">\n",
    "\n",
    "<TABLE>\n",
    "<TR>\n",
    "<TD colspan=\"2\" align=\"left\" valign=\"top\" style=\"min-width:400px;\">\n",
    "<A HREF=\"../soft.phtml?id=748187\" class=\"blueb\">ルートダブル - Before Crime ＊ After Days - </A><a href=\"https://order.zams.biz/comike/mypage/af_wish_list.phtml?action=add&id=748187\" title=\"お気に入りに追加\"><IMG class=\"lazy\" src=\"/common/images/space.gif\" data-original=\"/common/images/favorite.gif\" class=\"favorite\"></a><a href=\"/php/search.phtml?search_reference_id=748187\" title=\"この商品の関連を開く\"><IMG class=\"lazy\" src=\"/common/images/space.gif\" data-original=\"/common/images/relation.gif\" class=\"relation\"></a>\n",
    "</TD></TR>\n",
    "<TR><TD align=\"left\" valign=\"top\" width=\"100%\" style=\"line-height:1.35;\">\n",
    "<p><span class=\"orangeb\">[PCゲーム・一般]</span><br>\n",
    "発売日：2012/09/28<!--発売日--><BR>\n",
    "ブランド名： <A href=\"http://www.yetigame.jp/\" class=\"blue\" target=\"_blank\" >イエティ</A><!--BRAND--><BR>\n",
    "メディア： DVD-ROM<!--MEDIA--><BR>\n",
    "定価：￥8,800 (税込￥9,504)<!--PRICE--><BR>\n",
    "<BR>\n",
    "\n",
    "\n",
    "</p>\n",
    "\n",
    "</TD>\n",
    "<TD align=\"center\">\n",
    "<div class=\"pickup_block\">\n",
    "<p></p><!--comike_pickup-->\n",
    "</div>\n",
    "</TD>\n",
    "</TR>\n",
    "</TABLE>\n",
    "\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</li>\n",
    "'''\n",
    "\n",
    "link = \"http://www.getchu.com/php/search.phtml?search_keyword=&search_title=&search_brand=&search_person=\" + urllib.quote(key3) + \"&search_jan=&search_isbn=&genre=all&start_date=&end_date=&age=&list_count=30&sort=sales&sort2=down&list_type=list&search=1&pageID=1\"#+ str(page)\n",
    "'''    \n",
    "res = requests.get(link)\n",
    "res.encoding =  res.apparent_encoding\n",
    "    \n",
    "only_a_tags = SoupStrainer(\"ul\", class_=\"display\")#縮小處理範圍\n",
    "#先html.parser解析與縮小範圍，再以字串給lxml\n",
    "#soup = BeautifulSoup(str(BeautifulSoup(res.text,\"html.parser\",  parse_only=only_a_tags)),\"lxml\")\n",
    "sop = BeautifulSoup(res.text,\"html.parser\",  parse_only=only_a_tags)\n",
    "#print sop\n",
    "a=0\n",
    "for li in sop.select('li'):\n",
    "    soop=''\n",
    "    #li = '<html><body>'+str(li)+'</body></html>'\n",
    "    #print len(li),li\n",
    "    soop = BeautifulSoup(str(li),\"lxml\")\n",
    "    ctype = soop.select('.orangeb')[0].text\n",
    "    cbook = soop.select('.blueb')[0].text\n",
    "    cdata = soop.select('.orangeb')[0].next_sibling.next_sibling[5:]\n",
    "    print a,ctype,cbook,cdata\n",
    "    a=a+1\n",
    "#'''\n",
    "soup = BeautifulSoup(html_doc,\"lxml\")\n",
    "print soup.select('.orangeb')[0].next_sibling.next_sibling[5:]\n",
    "a=['q','ww','e','r']\n",
    "b= 'w'\n",
    "if b in a :\n",
    "    print 'ok'\n",
    "blink = ''\n",
    "blink = soup.select('.blueb')[0].get('href')\n",
    "print blink , len(str(soup))\n",
    "blink = mlink + blink[3:]\n",
    "print blink\n",
    "c='0123456789'\n",
    "c=c[:3]+'aaa'+c[3:]\n",
    "print c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
