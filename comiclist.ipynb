{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIG\n",
      "上藤政樹 85 num\n",
      "========\n",
      "page:1\n",
      "page:2\n",
      "page:3\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "import urllib , requests , sys ,string\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "\n",
    "#comiclist\n",
    "#fkey作者來源key.txt\n",
    "#fcomiclist結果key_comiclist.text\n",
    "\n",
    "#頁顯示數量\n",
    "pnum = 30\n",
    "\n",
    "fkey = open('key.txt', 'r')\n",
    "key = fkey.readline()#key=作者\n",
    "fkey.close()\n",
    "\n",
    "#key=作者\n",
    "#key2 = urllib.quote(key.decode('utf8').encode('sjis'))\n",
    "#key3 = urllib.unquote(key2.decode('sjis').encode('utf8'))\n",
    "#網址用\n",
    "#key4 = urllib.quote(key)\n",
    "#print key4\n",
    "\n",
    "#檢查BOM\n",
    "if '%EF%BB%BF' in urllib.quote(key):\n",
    "    print 'fuck ms'\n",
    "\n",
    "link = \"http://comiclist.jp/index.php?p=s&mode=ss&keyword=\" + urllib.quote(key) + \"&type=title\"\n",
    "res = requests.get(link)\n",
    "\n",
    "res.encoding =  res.apparent_encoding#亂碼處理\n",
    "only_a_tags = SoupStrainer(id='listArea')#縮小檢索範圍\n",
    "soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)#.prettify()#prettify_縮進顯示html\n",
    "\n",
    "def next(page = 2):\n",
    "    #http://comiclist.jp/index.php?p=s&mode=ss&type=title&keyword=%E4%B8%8A%E8%97%A4%E6%94%BF%E6%A8%B9&andor=and&maxline=30&pgn=3&pgn=1\n",
    "    #&andor=and&maxline=無影響&pgn=無影響&pgn=頁\n",
    "    link = \"http://comiclist.jp/index.php?p=s&mode=ss&type=title&keyword=\" + urllib.quote(key) + \"&andor=and&maxline=30&pgn=1&pgn=\" + str(page)\n",
    "    res = requests.get(link)\n",
    "    res.encoding =  res.apparent_encoding\n",
    "    only_a_tags = SoupStrainer(id='listArea')\n",
    "    soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)#.prettify()\n",
    "    return soup\n",
    "\n",
    "#資料處理\n",
    "def findbook(soup , page = 1):\n",
    "    a =0\n",
    "    for strong in soup.select('strong'):\n",
    "            #print soup.select('strong')[a].text\n",
    "            cname = soup.select('.list-name')[a].text\n",
    "            cbook = soup.select('strong')[a].text\n",
    "            cdata = soup.select('.list-day')[a].text\n",
    "            \n",
    "            #作者處理\n",
    "            check = 0#非目標作者\n",
    "            cname = cname.replace(u'　','')#移除全形空格\n",
    "            ##while對split切不出不視為陣列？\n",
    "            if u'］' not in cname:\n",
    "                cname = cname + u'］'\n",
    "            sname = cname.split(u'］')#切拆作者\n",
    "            #print sname[b]\n",
    "            \n",
    "            #'''\n",
    "            b = 0\n",
    "            name = ''\n",
    "            while len(sname[b]) > 0:\n",
    "            #while sname[b]:\n",
    "                #s_name = sname[b]\n",
    "                ssname = sname[b].split(u'［')#切拆作者後綴\n",
    "                \n",
    "                tname = ssname[0]\n",
    "                #print len(key) , key , len(temp.encode('utf8')) , temp.encode('utf8')\n",
    "                \n",
    "                #if key == ssname[0]:\n",
    "                #符合作者\n",
    "                if key == tname.encode('utf8'):\n",
    "                    check = 1\n",
    "                    #符合後綴\n",
    "                    if (u'画' in ssname[1] )or (u'著' in ssname[1] ):\n",
    "                        check = 2\n",
    "                        #print 'ok'\n",
    "                        \n",
    "                b = b + 1\n",
    "                #ssname[0] =  u'、' + ssname[0]\n",
    "                #作者疊加\n",
    "                name = name + ssname[0] + u'、'\n",
    "                #print ssname[0]\n",
    "            #'''\n",
    "            #多於分號處理\n",
    "            name = name.rstrip(u'、')\n",
    "            #print name\n",
    "            \n",
    "            #單作者檢查\n",
    "            '''\n",
    "            if soup.select('.list-name')[a].br:\n",
    "                #print '多作者處理'\n",
    "            '''            \n",
    "            \n",
    "            #書名\n",
    "            book = cbook\n",
    "            if (u'（成）' in cbook) and (check == 2):\n",
    "                check = 3\n",
    "                book = cbook.replace(u'（成）','')\n",
    "                #print 'adult' \n",
    "            #print cbook\n",
    "            \n",
    "            #日期\n",
    "            #print soup.select('.list-day')[a]\n",
    "            #print cdata\n",
    "            cdata = cdata.rstrip()\n",
    "            if len(cdata) < 8:\n",
    "                cdata = '0000/00/00'\n",
    "                check = 4\n",
    "            #print cbook + 'check' + str(check) +'\\n' + cdata\n",
    "            \n",
    "            #重複日期判斷\n",
    "            data = cdata[:10]\n",
    "            while listdata.count(data):\n",
    "            #for data in listdata:\n",
    "                #日期+1_十位數填0\n",
    "                data = data[:8] + str(int(data[8:]) + 1).rjust(2,'0')\n",
    "            listdata.append(data)\n",
    "            \n",
    "            #卷\n",
    "            if (u'巻' in cdata) :#and (check > 0):\n",
    "            #if soup.select('.list-day')[a].br:\n",
    "                #print '卷處理' + str(cdata[10:-1])\n",
    "                #冊數\n",
    "                bnum = cdata[10:-1]\n",
    "                book = book + '[' + bnum + ']'\n",
    "                #listbnum.append(book)\n",
    "            #print name.encode('utf8') , urllib.quote(key)\n",
    "            #print urllib.quote(name.encode('utf8'))\n",
    "            \n",
    "            #存檔處理\n",
    "            #if key == name.encode('utf8'):\n",
    "            if check > 0:\n",
    "                #print key ,  sname[0]\n",
    "                #print soup.select('.list-day')[a].text , soup.select('.list-name')[a].text\n",
    "                #fout.write(soup.select('.list-day')[a].text.encode('utf8') + cbook.encode('utf8')  + '\\n')\n",
    "                \n",
    "                #fout.write(cdata.encode('utf8') + cbook.encode('utf8')  + '\\n')\n",
    "                #字典新增\n",
    "                #dict1.setdefault('b',2)\n",
    "                if check == 1:\n",
    "                    dict1.setdefault(data,book)\n",
    "                elif check ==2:\n",
    "                    dict2.setdefault(data,book)\n",
    "                elif check ==3:\n",
    "                    dict3.setdefault(data,book)\n",
    "                elif check ==4:\n",
    "                    dict4.setdefault(data,book)\n",
    "            #print key ,  sname[0]\n",
    "            \n",
    "            \n",
    "            \n",
    "            #fout.write(soup.select('strong')[a].text.encode('utf8') + '\\n')\n",
    "            \n",
    "            a = a + 1\n",
    "            #print a\n",
    "    #print listdata[:]\n",
    "    #print '========'\n",
    "            \n",
    "\n",
    "#資料筆數_是否數字\n",
    "if soup.find_all('b')[1].text.isdigit():\n",
    "#if soup.find_all('b')[1].text > 0:\n",
    "    #print soup.find_all('b')\n",
    "    if int(soup.find_all('b')[1].text) > pnum:\n",
    "        print 'BIG'\n",
    "    \n",
    "    fout = open(key.decode('utf8') + '_comiclist.txt', 'w')\n",
    "    fout.write('comiclist\\n')\n",
    "    print key , soup.find_all('b')[1].text , 'num\\n========'\n",
    "    fout.write('!' + key + '\\n!總筆數' + soup.find_all('b')[1].text.encode('utf8') + '\\n')\n",
    "    #fout.write('!' + key.encode('utf8') + '\\n!總筆數'+soup.find_all('b')[1].encode('utf8')+'\\n')\n",
    "    \n",
    "    p = 0#頁\n",
    "    #建空輸出用字典與陣列\n",
    "    dict1={}#作者無後綴\n",
    "    dict2={}#作者一般向青年向\n",
    "    dict3={}#作者成人向\n",
    "    dict4={}#作者成人向新刊\n",
    "    listbnum=[]\n",
    "    bnum = ''#卷\n",
    "    listdata = []\n",
    "    #num = soup.find_all('b')[1].text\n",
    "    \n",
    "    ##\n",
    "    while (int(soup.find_all('b')[1].text) - p * pnum) > 0:\n",
    "        #ggg(p)\n",
    "        p = p + 1\n",
    "        print 'page:' + str(p)\n",
    "        soup = next(p)\n",
    "        findbook(soup)\n",
    "    \n",
    "    #日期排序\n",
    "    listdata.sort()\n",
    "    \n",
    "    temp = ''\n",
    "    #成人向輸出dict3\n",
    "    fout.write('==adult_' + str(len(dict3)) +'_成人向\\n')\n",
    "    for temp in listdata:\n",
    "    #for temp in dict3.itervalues():\n",
    "        if dict3.get(temp):\n",
    "            fout.write(temp.encode('utf8') +dict3[temp].encode('utf8')  + '\\n')\n",
    "            #print temp , dict3[temp]\n",
    "            if dict3[temp].count(u']',-2):\n",
    "            #if u']' in dict3[temp]:\n",
    "                listbnum.append(dict3[temp])\n",
    "    \n",
    "    #輸出多卷\n",
    "    #for temp in listbnum:\n",
    "    fout.write('==adultnum_' + str(len(listbnum)) +'_成人向多卷\\n')\n",
    "    for temp in listbnum:\n",
    "        #print str(listbnum[:]).encode('utf8')\n",
    "        #print temp\n",
    "        fout.write(temp.encode('utf8') + '\\n')\n",
    "    \n",
    "    #新刊輸出dict4\n",
    "    fout.write('==new_' + str(len(dict4)) +'_新\\n')\n",
    "    for temp in listdata:\n",
    "    #for temp in dict3.itervalues():\n",
    "        if dict4.get(temp):\n",
    "            fout.write(temp.encode('utf8') +dict4[temp].encode('utf8')  + '\\n')\n",
    "            #print temp , dict3[temp]\n",
    "    \n",
    "    #一般向dict2\n",
    "    fout.write('==nomal_' + str(len(dict2)) +'_一般向青年向\\n')\n",
    "    for temp in listdata:\n",
    "    #for temp in dict3.itervalues():\n",
    "        if dict2.get(temp):\n",
    "            fout.write(temp.encode('utf8') +dict2[temp].encode('utf8')  + '\\n')\n",
    "            #print temp , dict3[temp]\n",
    "    \n",
    "    #其他輸出dict1\n",
    "    fout.write('==unknow_' + str(len(dict1)) +'_不明\\n')\n",
    "    for temp in listdata:\n",
    "    #for temp in dict3.itervalues():\n",
    "        if dict1.get(temp):\n",
    "            fout.write(temp.encode('utf8') +dict1[temp].encode('utf8')  + '\\n')\n",
    "            #print temp , dict3[temp]\n",
    "    \n",
    "    \n",
    "    fout.close()\n",
    "    print 'ok'\n",
    "elif soup.find_all('b')[1].text:\n",
    "    print '同人'\n",
    "#print soup.select('.search_page_link_info')[1].text\n",
    "#print soup.select('.search_page_link_info')[1]\n",
    "\n",
    "#爬蟲重點內容\n",
    "#print soup.select('#listBOX-search')[0].text\n",
    "\n",
    "#print soup.select('td[class^=\"list-line\"]')\n",
    "#print res.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
