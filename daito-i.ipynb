{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "みちきんぐ 17 num\n",
      "========v1\n",
      "page:0\n",
      "16 .\n",
      "ok\n",
      "3 .. 2 .. 1 ..\n",
      "Press Any Key To Exit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "import urllib , requests , sys ,string ,time\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "from time import gmtime, strftime\n",
    "\n",
    "#daito-i\n",
    "#fkey作者來源key.txt\n",
    "#fcomiclist結果key_daito-i.text\n",
    "\n",
    "#輸出格式：\n",
    "#daito-i\n",
    "#!作者\n",
    "#!總筆數\n",
    "#==類別_數量_中文敘述\n",
    "#條目\n",
    "#!網址\n",
    "\n",
    "#comiclist常數\n",
    "pnum = 30#頁顯示數量\n",
    "mlink = 'http://www.daito-i.com/top/'#前綴網址\n",
    "\n",
    "fkey = open('key.txt', 'r')\n",
    "key = fkey.readline()#key=作者\n",
    "fkey.close()\n",
    "\n",
    "#檢查BOM\n",
    "if '%EF%BB%BF' in urllib.quote(key):\n",
    "    print 'fuck ms'\n",
    "\n",
    "'''\n",
    "link = \"http://comiclist.jp/index.php?p=s&mode=ss&keyword=\" + urllib.quote(key) + \"&type=title\"\n",
    "res = requests.get(link)\n",
    "'''\n",
    "\n",
    "\n",
    "#print key2\n",
    "#print key3\n",
    "adict = { \n",
    "'mode':'search',\n",
    "'page_num':'0',\n",
    "'search_cat':'',\n",
    "'keyword':key\n",
    "}\n",
    "res = requests.post(\"http://www.daito-i.com/top/show_unit.php\", data = adict)\n",
    "\n",
    "res.encoding =  res.apparent_encoding#亂碼處理\n",
    "only_a_tags = SoupStrainer(id='contents')#縮小檢索範圍\n",
    "soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)#.prettify()#prettify_縮進顯示html\n",
    "\n",
    "#換頁\n",
    "def next(page = 2):\n",
    "    #http://comiclist.jp/index.php?p=s&mode=ss&type=title&keyword=%E4%B8%8A%E8%97%A4%E6%94%BF%E6%A8%B9&andor=and&maxline=30&pgn=3&pgn=1\n",
    "    #&andor=and&maxline=無影響&pgn=無影響&pgn=頁\n",
    "    #link = \"http://comiclist.jp/index.php?p=s&mode=ss&type=title&keyword=\" + urllib.quote(key) + \"&andor=and&maxline=30&pgn=1&pgn=\" + str(page)\n",
    "    adict = { \n",
    "        'mode':'search',\n",
    "        'category':'',\n",
    "        'subcategory':'',\n",
    "        'search_cat':'',\n",
    "        'keyword':key,\n",
    "        'sort':'',\n",
    "        'page_num':str(page)\n",
    "    }\n",
    "    res = requests.post(\"http://www.daito-i.com/top/show_unit.php\", data = adict)\n",
    "    res.encoding =  res.apparent_encoding\n",
    "    only_a_tags = SoupStrainer(id='contents')\n",
    "    soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)#.prettify()\n",
    "    return soup\n",
    "\n",
    "#全轉半\n",
    "def Q2B(ustring):\n",
    "    fs=u'０１２３４５６７８９ＱｑＷｗＥｅＲｒＴｔＹｙＵｕＩｉＯｏＰｐＡａＳｓＤｄＦｆＧｇＨｈＪｊＫｋＬｌＺｚＸｘＣｃＶｖＢｂＮｎＭｍ'\n",
    "    hs=u'0123456789QqWwEeRrTtYyUuIiOoPpAaSsDdFfGgHhJjKkLlZzXxCcVvBbNnMm'\n",
    "    rstr = ''\n",
    "    for tm in ustring:\n",
    "        if fs.find(tm)+1:\n",
    "            tm = hs[fs.find(tm)]\n",
    "        rstr = rstr + tm\n",
    "    ustring = rstr\n",
    "    ustring=ustring.lower()\n",
    "    return ustring\n",
    "#半轉全\n",
    "def B2Q(ustring):\n",
    "    fs=u'０１２３４５６７８９ＱｑＷｗＥｅＲｒＴｔＹｙＵｕＩｉＯｏＰｐＡａＳｓＤｄＦｆＧｇＨｈＪｊＫｋＬｌＺｚＸｘＣｃＶｖＢｂＮｎＭｍ'\n",
    "    hs=u'0123456789QqWwEeRrTtYyUuIiOoPpAaSsDdFfGgHhJjKkLlZzXxCcVvBbNnMm'\n",
    "    rstr = ''\n",
    "    ustring=ustring.lower()\n",
    "    for tm in ustring:\n",
    "        if hs.find(tm)+1:\n",
    "            tm = fs[hs.find(tm)]\n",
    "        rstr = rstr + tm\n",
    "    ustring = rstr\n",
    "    return ustring\n",
    "\n",
    "#資料儲存\n",
    "def save(sdict , check=0):\n",
    "    #print listdata\n",
    "    for temp in listdata:\n",
    "        if sdict.get(temp):\n",
    "            #fout.write(temp.encode('utf8') +sdict[temp].encode('utf8')  + '\\n')\n",
    "            fout.write(sdict[temp].encode('utf8')  + '\\n')\n",
    "    #return\n",
    "\n",
    "#新刊\n",
    "def find_1():\n",
    "    #收網址\n",
    "    #網址get\n",
    "    #找出目錄_找出篇名\n",
    "    #回報目錄篇名\n",
    "    print 'find1'\n",
    "    return 'a'\n",
    "#單行本\n",
    "def find_2(blink='http://www.daito-i.com/top/'):\n",
    "    global bdict2\n",
    "    bdate = ''\n",
    "    res = requests.get(blink)\n",
    "    res.encoding =  res.apparent_encoding\n",
    "    only_a_tags = SoupStrainer(class_=\"rec\")\n",
    "    soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)\n",
    "    #print soup.select('strong')[0].text,soup.select('strong')[1].text#書名_作者\n",
    "    #print len(soup.select('.goodTxt')),soup.select('.goodTxt')[0]#主內容\n",
    "    \n",
    "    nnum = str(soup.select('.goodTxt')[0]).count('<br/>')#<br/>次數\n",
    "    for tm in soup.select('.goodTxt')[0].select('p'):\n",
    "        if (str(tm).count('<br/>') > 5):# and str(tm).count(key):\n",
    "            #print tm.text,type(tm.text)\n",
    "            bdate = bdate + '(' + str(str(tm).count('<br/>')) + ')-'#單行本話數\n",
    "            bdict2 = bdict2 + tm.get_text('_') + '_'#比對用疊加\n",
    "            tm = tm.get_text('\\n-')\n",
    "            bdate = bdate + tm + '%\\n'\n",
    "    #print 'find2'\n",
    "    bdate = bdate[:bdate.find(u'%')+1]#去多餘\n",
    "    return bdate,nnum\n",
    "#雜誌類\n",
    "def find_3(blink='http://www.daito-i.com/top/'):\n",
    "    bdate = ''\n",
    "    res = requests.get(blink)\n",
    "    res.encoding =  res.apparent_encoding\n",
    "    only_a_tags = SoupStrainer(class_=\"rec\")\n",
    "    soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)\n",
    "    #print soup.select('strong')[0].text,soup.select('strong')[1].text#雜誌名_期號\n",
    "    #print len(soup.select('.goodTxt')),soup.select('.goodTxt')[0]#主內容\n",
    "    \n",
    "    nnum = Q2B(soup.select('.goodTxt')[0].text).encode('utf8').count(key)#作者出現次數\n",
    "    for tm in soup.select('.goodTxt')[0].select('p'):\n",
    "        if (str(tm).count('】') > 3):# and str(tm).count(key):#特徵與作者\n",
    "            #print tm.text,type(tm.text)\n",
    "            tm=tm.text\n",
    "            tm=Q2B(tm)\n",
    "            while tm.find(u'【') > 0:#特徵存在\n",
    "                if key.decode('utf8') in tm[tm.find(u'【')+1:tm.find(u'】')]:#符合作者\n",
    "                    listdict3.append(tm[:tm.find(u'【')])#比對用疊加\n",
    "                    bdate = bdate + tm[:tm.find(u'【')] + '_'#疊加\n",
    "                    if key.decode('utf8') != tm[tm.find(u'【')+1:tm.find(u'】')]:#作者不唯一\n",
    "                        bdate = bdate + tm[tm.find(u'【'):tm.find(u'】')+1] + '_'#疊加\n",
    "                tm=tm[tm.find(u'】')+1:]\n",
    "    #print 'find3'\n",
    "    return bdate,nnum\n",
    "\n",
    "#資料處理\n",
    "def findbook(soup , page = 1):\n",
    "    global pnn\n",
    "    a =0\n",
    "    for itmBox in soup.select('.itmBox'):\n",
    "        ctype = soup.select('.marks')[a].next_sibling[1:]\n",
    "        cbook = soup.select('.itmBox')[a].select('a')[1].text\n",
    "        cname = soup.select('.itmBox')[a].select('a')[2].text\n",
    "        #print type(itmBox),itmBox\n",
    "        #print pnn,ctype,cbook,cname\n",
    "        \n",
    "        #作品網址\n",
    "        blink = ''\n",
    "        blink = soup.select('.itmBox')[a].select('a')[0].get('href')\n",
    "        blink = mlink + blink\n",
    "        #print blink\n",
    "        \n",
    "        #書名\n",
    "        \n",
    "        #類型\n",
    "        bdate = ''#次層頁面主資料\n",
    "        #b = str(pnn).rjust(3,'0')#倒數的流水號\n",
    "        \n",
    "        #予約商品_新刊\n",
    "        if ctype in u'予約商品':#予約商品_\n",
    "            #bdate=find_1()\n",
    "            book = cname + '_' + cbook + '_\\n!' + blink\n",
    "            dict1.setdefault(pnn,book)\n",
    "        #コミックス_單行本\n",
    "        elif ctype in u'コミックス':\n",
    "            if key.decode('utf8') in Q2B(itmBox.text):\n",
    "                cfind=find_2(blink)#目標作者的單行本\n",
    "                bdate = cfind[0]\n",
    "                if len(bdate) < 10:\n",
    "                    bdate = 'lost'\n",
    "                book = cbook + '_\\n' + bdate + '\\n!' + blink\n",
    "                dict2.setdefault(pnn,book)\n",
    "            elif cname in u'アンソロジー ':\n",
    "                cfind=find_3(blink)#非作者or合本_アンソロジー \n",
    "                bdate=cfind[0]\n",
    "                book = cname + '_' + cbook + '_' + bdate + '_\\n!' + str(cfind[1]) + '!' + blink\n",
    "                dict2.setdefault(pnn,book)\n",
    "            else:\n",
    "                book = ctype + '_' + cname + '_' + cbook + '_\\n!' + blink\n",
    "                dict2.setdefault(pnn,book)\n",
    "        #雑誌_雜誌單篇\n",
    "        elif ctype in u'雑誌':\n",
    "            cfind=find_3(blink)\n",
    "            bdate=cfind[0]\n",
    "            book = cbook + '[' + Q2B(cname) + ']_' + bdate + '_\\n!' + str(cfind[1]) + '!' + blink\n",
    "            dict3.setdefault(pnn,book)\n",
    "        #ノベルズ_文庫_画集_書籍_\n",
    "        elif ctype in [u'ノベルズ',u'文庫',u'画集',u'書籍']:\n",
    "            book = ctype + '_' + cname + '_' + cbook + '_\\n!' + blink\n",
    "            dict4.setdefault(pnn,book)\n",
    "        #ムック_同人誌_販促品_\n",
    "        elif ctype in [u'ムック',u'同人誌',u'販促品']:\n",
    "            book = ctype + '_' + cname + '_' + cbook + '_\\n!' + blink\n",
    "            dict5.setdefault(pnn,book)\n",
    "        #print cfind[1],'num'\n",
    "        #\n",
    "        listdata.append(pnn)\n",
    "        '''\n",
    "        1予約商品_\n",
    "        2コミックス_\n",
    "        3雑誌_\n",
    "        4ノベルズ_文庫_画集_書籍_\n",
    "        5ムック_同人誌_販促品_\n",
    "        '''\n",
    "        \n",
    "        a = a + 1\n",
    "        \n",
    "        #特徵itmBox開滿每頁顯示才放資料，會跑出空值，以搜尋筆數跳出。\n",
    "        pnn = pnn -1\n",
    "        #print a,pnn,int(pnn)\n",
    "        #if pnn <78:\n",
    "        if pnn == 0:\n",
    "            break\n",
    "        \n",
    "        print '\\r',a,\n",
    "    print '.'\n",
    "    #print '========'\n",
    "\n",
    "########\n",
    "key=key.lower()\n",
    "KEY = B2Q(key.decode('utf8'))\n",
    "KEY= KEY.encode('utf8')\n",
    "\n",
    "pn =''\n",
    "pn = soup.select('tr')[1].select('td')[1].text\n",
    "pn = pn[0:pn.find(u'件')]#筆數\n",
    "\n",
    "#資料筆數_是否數字\n",
    "if pn > 0:\n",
    "    if int(pn) > pnum:\n",
    "        print 'BIG'\n",
    "    \n",
    "    fout = open('output/'+key.decode('utf8') + '_daito-iv1.txt', 'w')#寫入模式開檔\n",
    "    fout.write('daito-i\\n')#comiclist\n",
    "    print key.decode('utf8') , pn , 'num\\n========v1'\n",
    "    time.sleep(1)\n",
    "    fout.write('!' + key + '\\n!總筆數' + pn.encode('utf8') +'_'+ strftime(\"%Y/%m/%d,%H:%M\")+'->')\n",
    "    \n",
    "    p = 0#頁\n",
    "    #建空輸出用字典與陣列\n",
    "    dict1={}#新刊\n",
    "    dict2={}#單行本\n",
    "    dict3={}#雜誌\n",
    "    dict4={}#作畫擔任\n",
    "    dict5={}#其他\n",
    "    listdict3=[]#雜誌單篇\n",
    "    bdict2 = ''#單行本書目\n",
    "    listdata = []\n",
    "    #搜尋結果特徵碼部分有空值?計步避開\n",
    "    pnn = int(pn)#計步\n",
    "    \n",
    "    #資料處理\n",
    "    while (int(pn) - p * pnum) > 0:\n",
    "        \n",
    "        print 'page:' + str(p)\n",
    "        soup = next(p)#頁\n",
    "        findbook(soup)#資料處理\n",
    "        p = p + 1\n",
    "        time.sleep(1)\n",
    "    \n",
    "    #日期排序\n",
    "    listdata.sort()\n",
    "    \n",
    "    #sys.exit()################\n",
    "    fout.write(strftime(\"%H:%M\")+'\\n')\n",
    "    temp = ''\n",
    "    #dict1_雜誌輸出\n",
    "    fout.write('==new_' + str(len(dict1)) +'_新刊\\n')\n",
    "    save(dict1)\n",
    "    #dict2_單行本輸出\n",
    "    fout.write('==book_' + str(len(dict2)) +'_單行本\\n')\n",
    "    save(dict2)\n",
    "    #dict3_雜誌輸出\n",
    "    fout.write('==adult_' + str(len(dict3)) +'_雜誌\\n')\n",
    "    save(dict3)\n",
    "    #dict4_作畫擔任輸出\n",
    "    fout.write('==art_' + str(len(dict4)) +'_作畫擔任\\n')\n",
    "    save(dict4)\n",
    "    #dict5_其他輸出\n",
    "    fout.write('==other_' + str(len(dict5)) +'_其他\\n')\n",
    "    save(dict5)\n",
    "    \n",
    "    #單行本與雜誌比對\n",
    "    fout.write('==fd32_' + str(len(listdict3)) +'_比對結果\\n')\n",
    "    tmp = ''\n",
    "    out32 = ''\n",
    "    #listdict3_bdict2\n",
    "    for tmp in listdict3:\n",
    "        #print type(tmp),type(bdict2)\n",
    "        if tmp in bdict2:\n",
    "            #print 'catch'\n",
    "            tmp='-'+tmp\n",
    "        out32=out32+tmp+'\\n'\n",
    "    fout.write(out32.encode('utf8'))\n",
    "    \n",
    "    fout.close()\n",
    "    print 'ok'\n",
    "elif p:\n",
    "    print '同人'\n",
    "\n",
    "#結束讀秒\n",
    "x=3\n",
    "while x!=0:\n",
    "    print x,'..',\n",
    "    x=x-1\n",
    "    time.sleep(1)\n",
    "raw_input(\"\\nPress Any Key To Exit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フリージング２８ 金光鉉／原作：林達永&nbsp&nbsp\n",
      "第１９０話　拒絶Ｉ第１９１話　拒絶ＩＩ第１９２話　拒絶ＩＩＩ第１９３話　役割Ｉ第１９４話　役割ＩＩ第１９５話　価値第１９６話　相応の力 <type 'unicode'>\n",
      "<type 'unicode'>\n",
      "(6)-第１９０話　拒絶Ｉ\n",
      "-第１９１話　拒絶ＩＩ\n",
      "-第１９２話　拒絶ＩＩＩ\n",
      "-第１９３話　役割Ｉ\n",
      "-第１９４話　役割ＩＩ\n",
      "-第１９５話　価値\n",
      "-第１９６話　相応の力%\n",
      "= 11\n",
      "ok\n",
      "<type 'str'>\n",
      "<type 'str'> <type 'str'>\n",
      "YASUDA ＹＡＳＵＤＡ ｑｗｅｒ３４５\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '2011\\xe5\\xb9\\xb412\\xe6\\x9c\\x88\\xe5\\x8f\\xb7'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-d3d52d370b44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKEY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mKEY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mB2Q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'qwer345'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2011年12月号'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '2011\\xe5\\xb9\\xb412\\xe6\\x9c\\x88\\xe5\\x8f\\xb7'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "import urllib , requests , sys ,string ,time\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "\n",
    "fkey = open('key.txt', 'r')\n",
    "key = fkey.readline()#key=作者\n",
    "fkey.close()\n",
    "\n",
    "#檢查BOM\n",
    "if '%EF%BB%BF' in urllib.quote(key):\n",
    "    print 'fuck ms'\n",
    "\n",
    "'''\n",
    "link = \"http://comiclist.jp/index.php?p=s&mode=ss&keyword=\" + urllib.quote(key) + \"&type=title\"\n",
    "res = requests.get(link)\n",
    "'''\n",
    "#當sjis輸出utf8的url\n",
    "key2 = urllib.quote(key.decode('utf8').encode('sjis'))\n",
    "#utf8的url翻sjis\n",
    "key3 = urllib.unquote(key2.decode('sjis').encode('utf8'))\n",
    "#虎穴用sjis\n",
    "\n",
    "#print key2\n",
    "#print key3\n",
    "adict = { \n",
    "'mode':'search',\n",
    "'page_num':'0',\n",
    "'search_cat':'',\n",
    "'keyword':key\n",
    "}\n",
    "'''\n",
    "res = requests.post(\"http://www.daito-i.com/top/show_unit.php\", data = adict)\n",
    "\n",
    "res.encoding =  res.apparent_encoding#亂碼處理\n",
    "only_a_tags = SoupStrainer(id='contents')#縮小檢索範圍 id=\"wrapper\"\n",
    "'''\n",
    "h1='''\n",
    "<li>\n",
    "<div class=\"itmBox\">\t<a href=\"magazine/detail.php?code=4910137600157\">\n",
    "\t<img src=\"images/magazine/max/2014/4910137600157.jpg\" alt=\"ＣＯＭＩＣ漫華鏡[２０１５年１月号]\"></a><br />\n",
    "<div class=\"marks\"></div>\n",
    "雑誌<br />\n",
    "「<a href=\"magazine/detail.php?code=4910137600157\">ＣＯＭＩＣ漫華鏡</a>\n",
    "」<br />\n",
    "<a href=\"show_unit.php?mode=search&disp_type=2&keyword=%EF%BC%92%EF%BC%90%EF%BC%91%EF%BC%95%E5%B9%B4%EF%BC%91%E6%9C%88%E5%8F%B7\">２０１５年１月号</a>&nbsp&nbsp<br />\n",
    "<span class=\"itmPrc\">400円</span>\n",
    "</div></li>\n",
    "'''\n",
    "\n",
    "h2='''\n",
    "<html>\n",
    "<body>\n",
    "<li>\n",
    "<div class=\"itmBox\"><a href=\"detail.php?code=9784894656833\">[詳細ページ]</a>\n",
    "<img src=\"../images/printing.gif\"><br><br />\n",
    "<div class=\"marks\"><img src=\"../img/new.gif\" alt=\"new\" align=\"absmidedle\" border=\"0\">&nbsp;</div>\n",
    "予約商品<br />\n",
    "「<a href=\"detail.php?code=9784894656833\">ガールズ・マジョリティー</a>\n",
    "」<br />\n",
    "<a href=\"../show_unit.php?mode=search&disp_type=2&keyword=%E9%AB%98%E5%B2%A1%E5%9F%BA%E6%96%87\">高岡基文</a>&nbsp&nbsp<br />\n",
    "<span class=\"itmPrc\">1,080円</span>\n",
    "</div></li>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "soup = ''\n",
    "sou = ''\n",
    "#soup = BeautifulSoup(res.text ,\"lxml\" ,  parse_only=only_a_tags)#.prettify()\n",
    "soup = BeautifulSoup(h1 ,\"lxml\")# ,  parse_only=only_a_tags)#.prettify()\n",
    "sou = BeautifulSoup(h2 ,\"lxml\")\n",
    "#print soup.select('.marks')[0].next_sibling[1:]\n",
    "#print sou.select('.marks')[0].next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling\n",
    "#print sou.a\n",
    "#print soup.select('a')[1].text,soup.select('a')[2].text\n",
    "#print sou.select('a')[1].text,sou.select('a')[2].text\n",
    "#\n",
    "#全轉半\n",
    "def Q2B(ustring):\n",
    "    fs=u'０１２３４５６７８９ＱｑＷｗＥｅＲｒＴｔＹｙＵｕＩｉＯｏＰｐＡａＳｓＤｄＦｆＧｇＨｈＪｊＫｋＬｌＺｚＸｘＣｃＶｖＢｂＮｎＭｍ'\n",
    "    hs=u'0123456789QqWwEeRrTtYyUuIiOoPpAaSsDdFfGgHhJjKkLlZzXxCcVvBbNnMm'\n",
    "    rstr = ''\n",
    "    for tm in ustring:\n",
    "        if fs.find(tm)+1:\n",
    "            tm = hs[fs.find(tm)]\n",
    "        rstr = rstr + tm\n",
    "    ustring = rstr\n",
    "    return ustring\n",
    "#半轉全\n",
    "def B2Q(ustring):\n",
    "    fs=u'０１２３４５６７８９ＱｑＷｗＥｅＲｒＴｔＹｙＵｕＩｉＯｏＰｐＡａＳｓＤｄＦｆＧｇＨｈＪｊＫｋＬｌＺｚＸｘＣｃＶｖＢｂＮｎＭｍ'\n",
    "    hs=u'0123456789QqWwEeRrTtYyUuIiOoPpAaSsDdFfGgHhJjKkLlZzXxCcVvBbNnMm'\n",
    "    rstr = ''\n",
    "    for tm in ustring:\n",
    "        if hs.find(tm)+1:\n",
    "            tm = fs[hs.find(tm)]\n",
    "        rstr = rstr + tm\n",
    "    ustring = rstr\n",
    "    return ustring\n",
    "\n",
    "def find_x(blink='http://www.daito-i.com/top/comics/detail.php?code=9784799207833'):\n",
    "    bdate = ''\n",
    "    res = requests.get(blink)\n",
    "    res.encoding =  res.apparent_encoding\n",
    "    only_a_tags = SoupStrainer(class_=\"rec\")\n",
    "    soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)\n",
    "    print soup.select('strong')[0].text,soup.select('strong')[1].text#書名_作者\n",
    "    #print len(soup.select('.goodTxt')),soup.select('.goodTxt')[0]#主內容\n",
    "    \n",
    "    nnum = str(soup.select('.goodTxt')[0]).count('<br/>')#換行出現次數\n",
    "    for tm in soup.select('.goodTxt')[0].select('p'):\n",
    "        #print tm.prettify()\n",
    "        if (str(tm).count('<br/>') > 5):# and str(tm).count(key):\n",
    "            print tm.text,type(tm.text)\n",
    "            bdate = bdate + '(' + str(str(tm).count('<br/>')) + ')-'\n",
    "            #tm=str(tm).replace('<br/>', '\\n')\n",
    "            tm = tm.get_text('\\n-')\n",
    "            print type(tm)\n",
    "            bdate = bdate + tm + '%\\n'\n",
    "            '''\n",
    "            while tm.find(u'【') > 0:\n",
    "                if key.decode('utf8') in tm[tm.find(u'【')+1:tm.find(u'】')]:\n",
    "                    bdate = bdate + tm[:tm.find(u'【')] + '_'\n",
    "                tm=tm[tm.find(u'】')+1:]\n",
    "            #'''\n",
    "    #print 'find3'\n",
    "    bdate = bdate[:bdate.find(u'%')+1]#去多餘\n",
    "    return bdate,nnum\n",
    "#print len(key.decode('utf8')) ,len(str(key)), find_3(),str(key)\n",
    "cb,bb=find_x()\n",
    "#print type(cb)\n",
    "print cb\n",
    "print '=',bb\n",
    "\n",
    "if 'sa' in 'gfdsa':\n",
    "    print  'ok'\n",
    "print type(key)\n",
    "KEY = B2Q(key.decode('utf8'))\n",
    "KEY= KEY.encode('utf8')\n",
    "print type(key), type(KEY)\n",
    "print key,KEY,B2Q('qwer345')\n",
    "print int('2011年12月号')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
