{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "\n",
      "\n",
      "87\n",
      "BIG\n",
      "鬼ノ仁 \n",
      "\n",
      "87 num\n",
      "========\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "import urllib , requests , sys ,string ,time\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "\n",
    "#daito-i\n",
    "#fkey作者來源key.txt\n",
    "#fcomiclist結果key_daito-i.text\n",
    "\n",
    "#輸出格式：\n",
    "#daito-i\n",
    "#!作者\n",
    "#!總筆數\n",
    "#==類別_數量_中文敘述\n",
    "#條目\n",
    "\n",
    "#comiclist常數\n",
    "pnum = 30#頁顯示數量\n",
    "mlink = 'http://www.daito-i.com/top/'#前綴網址\n",
    "\n",
    "fkey = open('key.txt', 'r')\n",
    "key = fkey.readline()#key=作者\n",
    "fkey.close()\n",
    "\n",
    "#檢查BOM\n",
    "if '%EF%BB%BF' in urllib.quote(key):\n",
    "    print 'fuck ms'\n",
    "\n",
    "'''\n",
    "link = \"http://comiclist.jp/index.php?p=s&mode=ss&keyword=\" + urllib.quote(key) + \"&type=title\"\n",
    "res = requests.get(link)\n",
    "'''\n",
    "#當sjis輸出utf8的url\n",
    "key2 = urllib.quote(key.decode('utf8').encode('sjis'))\n",
    "#utf8的url翻sjis\n",
    "key3 = urllib.unquote(key2.decode('sjis').encode('utf8'))\n",
    "#虎穴用sjis\n",
    "\n",
    "#print key2\n",
    "#print key3\n",
    "adict = { \n",
    "'mode':'search',\n",
    "'page_num':'0',\n",
    "'search_cat':'',\n",
    "'keyword':key\n",
    "}\n",
    "res = requests.post(\"http://www.daito-i.com/top/show_unit.php\", data = adict)\n",
    "\n",
    "res.encoding =  res.apparent_encoding#亂碼處理\n",
    "only_a_tags = SoupStrainer(id='contents')#縮小檢索範圍\n",
    "soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)#.prettify()#prettify_縮進顯示html\n",
    "\n",
    "#換頁\n",
    "def next(page = 2):\n",
    "    #http://comiclist.jp/index.php?p=s&mode=ss&type=title&keyword=%E4%B8%8A%E8%97%A4%E6%94%BF%E6%A8%B9&andor=and&maxline=30&pgn=3&pgn=1\n",
    "    #&andor=and&maxline=無影響&pgn=無影響&pgn=頁\n",
    "    #link = \"http://comiclist.jp/index.php?p=s&mode=ss&type=title&keyword=\" + urllib.quote(key) + \"&andor=and&maxline=30&pgn=1&pgn=\" + str(page)\n",
    "    adict = { \n",
    "        'mode':'search',\n",
    "        'category':'',\n",
    "        'subcategory':'',\n",
    "        'search_cat':'',\n",
    "        'keyword':key,\n",
    "        'sort':'',\n",
    "        'page_num':str(page)\n",
    "    }\n",
    "    res = requests.get(link)\n",
    "    res.encoding =  res.apparent_encoding\n",
    "    only_a_tags = SoupStrainer(id='contents')\n",
    "    soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)#.prettify()\n",
    "    return soup\n",
    "\n",
    "#全轉半\n",
    "def Q2B(ustring):\n",
    "    fs=u'０１２３４５６７８９ＱｑＷｗＥｅＲｒＴｔＹｙＵｕＩｉＯｏＰｐＡａＳｓＤｄＦｆＧｇＨｈＪｊＫｋＬｌＺｚＸｘＣｃＶｖＢｂＮｎＭｍ'\n",
    "    hs=u'0123456789QqWwEeRrTtYyUuIiOoPpAaSsDdFfGgHhJjKkLlZzXxCcVvBbNnMm'\n",
    "    rstr = ''\n",
    "    for tm in ustring:\n",
    "        if fs.find(tm)+1:\n",
    "            tm = hs[fs.find(tm)]\n",
    "        rstr = rstr + tm\n",
    "    ustring = rstr\n",
    "    return ustring\n",
    "\n",
    "#資料處理\n",
    "def findbook(soup , page = 1):\n",
    "    a =0\n",
    "    for strong in soup.select('strong'):\n",
    "            cname = soup.select('.list-name')[a].text\n",
    "            cbook = soup.select('strong')[a].text\n",
    "            #print soup.select('strong')[a]\n",
    "            cdata = soup.select('.list-day')[a].text\n",
    "            \n",
    "            #作者處理\n",
    "            check = 0#非目標作者\n",
    "            cname = cname.replace(u'　','')#移除全形空格\n",
    "            cname = Q2B(cname)\n",
    "            if u'］' not in cname:\n",
    "                cname = cname + u'］'\n",
    "            sname = cname.split(u'］')#切拆作者\n",
    "            \n",
    "            s_name = []\n",
    "            for temp in soup.select('.list-name')[a].select('a'):\n",
    "                temp = temp.text\n",
    "                temp = temp.replace(u'　','')#移除全形空格\n",
    "                temp = Q2B(temp)\n",
    "                s_name.append(temp)\n",
    "            s_name.append('')#避while錯誤\n",
    "            \n",
    "            b = 0\n",
    "            name = ''\n",
    "            while len(s_name[b]) > 0:\n",
    "                ##while對split切不出不視為陣列？\n",
    "                if u'［' not in s_name[b]:\n",
    "                    s_name[b] = s_name[b] + u'［'\n",
    "                ssname = s_name[b].split(u'［')#切拆作者後綴\n",
    "                #print ssname\n",
    "                tname = ssname[0]\n",
    "                if key == tname.encode('utf8'):#符合作者\n",
    "                    #print 'ok'\n",
    "                    check = 1#目標作者\n",
    "                    if (u'画' in ssname[1] )or (u'著' in ssname[1] ):#符合後綴\n",
    "                        check = 2#符合後綴\n",
    "                name = name + ssname[0] + u'、'#作者串接 \n",
    "                b = b + 1\n",
    "            name = name.rstrip(u'、')#多餘分號處理\n",
    "            \n",
    "            #書名處理\n",
    "            book = cbook\n",
    "            if (u'（成）' in cbook) and (check == 2):#去前綴\n",
    "                check = 3\n",
    "                book = cbook.replace(u'（成）','')\n",
    "            \n",
    "            #日期處理\n",
    "            cdata = cdata.rstrip()\n",
    "            if len(cdata) < 8:#無日期\n",
    "                cdata = '0000/00/00'#填入日期\n",
    "                check = 4#新作\n",
    "            data = cdata[:10]\n",
    "            while listdata.count(data):#重複日期判斷\n",
    "                data = data[:8] + str(int(data[8:]) + 1).rjust(2,'0')#日期+1_十位數填0\n",
    "            listdata.append(data)\n",
    "            \n",
    "            #多卷處理\n",
    "            if (u'巻' in cdata) and (int(cdata[10:-1]) > 1):\n",
    "                bnum = cdata[10:-1]#冊數\n",
    "                book = book + '[' + bnum + ']'\n",
    "            \n",
    "            #書名連結\n",
    "            blink = ''\n",
    "            blink = soup.select('.list-book')[a].select('a')[0].get('href')\n",
    "            \n",
    "            #字典新增\n",
    "            if check > 0:\n",
    "                if check == 1:\n",
    "                    book = book + '_' + name + '\\n' + mlink +blink#書名+作者+網址\n",
    "                    dict1.setdefault(data,book)\n",
    "                elif check ==2:\n",
    "                    dict2.setdefault(data,book)\n",
    "                elif check ==3:\n",
    "                    dict3.setdefault(data,book)\n",
    "                elif check ==4:\n",
    "                    dict4.setdefault(data,book)\n",
    "            a = a + 1\n",
    "            #print a\n",
    "    #print '========'\n",
    "\n",
    "########\n",
    "\n",
    "#if \n",
    "pn = soup.select('tr')[1].text\n",
    "\n",
    "print p.find(u'件')\n",
    "pn = pn[0:pn.find(u'件')]#筆數\n",
    "print pn\n",
    "\n",
    "#資料筆數_是否數字\n",
    "if pn > 0:\n",
    "    if int(pn) > pnum:\n",
    "        print 'BIG'\n",
    "    \n",
    "    fout = open(key.decode('utf8') + '_daito-i.txt', 'w')#寫入模式開檔\n",
    "    fout.write('daito-i\\n')#comiclist\n",
    "    print key.decode('utf8') , pn , 'num\\n========'\n",
    "    time.sleep(1)\n",
    "    fout.write('!' + key + '\\n!總筆數' + pn.encode('utf8') + '\\n')\n",
    "    sys.exit()\n",
    "    p = 0#頁\n",
    "    #建空輸出用字典與陣列\n",
    "    dict1={}#作者無後綴\n",
    "    dict2={}#作者一般向青年向\n",
    "    dict3={}#作者成人向\n",
    "    dict4={}#作者成人向新刊\n",
    "    listbnum=[]\n",
    "    bnum = ''#卷\n",
    "    listdata = []\n",
    "    \n",
    "    #資料處理\n",
    "    while (int(soup.find_all('b')[1].text) - p * pnum) > 0:\n",
    "        p = p + 1\n",
    "        print 'page:' + str(p)\n",
    "        soup = next(p)#頁\n",
    "        findbook(soup)#資料處理\n",
    "        time.sleep(1)\n",
    "    \n",
    "    #日期排序\n",
    "    listdata.sort()\n",
    "    \n",
    "    temp = ''\n",
    "    #dict3_成人向輸出\n",
    "    fout.write('==adult_' + str(len(dict3)) +'_成人向\\n')\n",
    "    for temp in listdata:\n",
    "        if dict3.get(temp):\n",
    "            fout.write(dict3[temp].encode('utf8')  + '\\n')\n",
    "            if dict3[temp].count(u']',-2):\n",
    "                listbnum.append(dict3[temp])\n",
    "    \n",
    "    #輸出多卷\n",
    "    fout.write('==adultnum_' + str(len(listbnum)) +'_成人向多卷\\n')\n",
    "    for temp in listbnum:\n",
    "        fout.write(temp.encode('utf8') + '\\n')\n",
    "    \n",
    "    #dict4_新刊輸出\n",
    "    fout.write('==new_' + str(len(dict4)) +'_新\\n')\n",
    "    for temp in listdata:\n",
    "        if dict4.get(temp):\n",
    "            fout.write(temp.encode('utf8') +dict4[temp].encode('utf8')  + '\\n')\n",
    "    \n",
    "    #dict2_一般向青年向\n",
    "    fout.write('==nomal_' + str(len(dict2)) +'_一般向青年向\\n')\n",
    "    for temp in listdata:\n",
    "        if dict2.get(temp):\n",
    "            fout.write(dict2[temp].encode('utf8')  + '\\n')\n",
    "    \n",
    "    #dict1_其他輸出\n",
    "    fout.write('==unknow_' + str(len(dict1)) +'_不明\\n')\n",
    "    for temp in listdata:\n",
    "        if dict1.get(temp):\n",
    "            fout.write(temp.encode('utf8') +dict1[temp].encode('utf8')  + '\\n')\n",
    "    \n",
    "    fout.close()\n",
    "    print 'ok'\n",
    "elif p:\n",
    "    print '同人'\n",
    "\n",
    "#結束讀秒\n",
    "x=3\n",
    "while x!=0:\n",
    "    print x,'..',\n",
    "    x=x-1\n",
    "    time.sleep(1)\n",
    "print 'end'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'unicode' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7a37d8a316c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m\"lxml\"\u001b[0m \u001b[1;33m,\u001b[0m  \u001b[0mparse_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0monly_a_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprettify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#soup = BeautifulSoup(res.text ,\"lxml\" ).prettify()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'unicode' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "import urllib , requests , sys ,string ,time\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "\n",
    "fkey = open('key.txt', 'r')\n",
    "key = fkey.readline()#key=作者\n",
    "fkey.close()\n",
    "\n",
    "#檢查BOM\n",
    "if '%EF%BB%BF' in urllib.quote(key):\n",
    "    print 'fuck ms'\n",
    "\n",
    "'''\n",
    "link = \"http://comiclist.jp/index.php?p=s&mode=ss&keyword=\" + urllib.quote(key) + \"&type=title\"\n",
    "res = requests.get(link)\n",
    "'''\n",
    "#當sjis輸出utf8的url\n",
    "key2 = urllib.quote(key.decode('utf8').encode('sjis'))\n",
    "#utf8的url翻sjis\n",
    "key3 = urllib.unquote(key2.decode('sjis').encode('utf8'))\n",
    "#虎穴用sjis\n",
    "\n",
    "#print key2\n",
    "#print key3\n",
    "adict = { \n",
    "'mode':'search',\n",
    "'page_num':'0',\n",
    "'search_cat':'',\n",
    "'keyword':key\n",
    "}\n",
    "res = requests.post(\"http://www.daito-i.com/top/show_unit.php\", data = adict)\n",
    "\n",
    "res.encoding =  res.apparent_encoding#亂碼處理\n",
    "only_a_tags = SoupStrainer(id='contents')#縮小檢索範圍 id=\"wrapper\"\n",
    "soup = BeautifulSoup(res.text ,\"lxml\" ,  parse_only=only_a_tags).prettify()\n",
    "#soup = BeautifulSoup(res.text ,\"lxml\" ).prettify()\n",
    "print soup.select('tr')[1].text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
