{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "こりす 30 num\n",
      "========\n",
      "page:0\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１５年５月号\n",
      "1 name_num\n",
      "雑誌 ＣＯＭＩＣ漫華鏡 ２０１５年１月号\n",
      "1 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１４年１０月号\n",
      "1 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１４年９月号\n",
      "1 name_num\n",
      "雑誌 ＣＯＭＩＣ失楽天 ２０１４年６月号\n",
      "1 name_num\n",
      "しちゃってもいいよ こりす&nbsp&nbsp\n",
      "11\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１４年１月号\n",
      "1 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１３年１２月号\n",
      "1 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１３年１１月号\n",
      "1 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１３年９月号\n",
      "1 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１３年８月号\n",
      "1 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１３年５月号\n",
      "2 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１３年４月号\n",
      "2 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１２年１１月号\n",
      "2 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１２年９月号\n",
      "1 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１２年７月号\n",
      "1 name_num\n",
      "ラブ・ペロ こりす&nbsp&nbsp\n",
      "15\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１２年５月号\n",
      "2 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１２年３月号\n",
      "1 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１２年２月号\n",
      "1 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１２年１月号\n",
      "1 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１１年９月号\n",
      "1 name_num\n",
      "雑誌 ＣＯＭＩＣポプリクラブ ２０１１年６月号\n",
      "1 name_num\n",
      "雑誌 コミックメガストアＨ ２０１０年１０月号\n",
      "1 name_num\n",
      "1 num\n",
      "雑誌 二次元ドリームマガジン ２０１０年２月号\n",
      "1 name_num\n",
      "1 num\n",
      "========\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "<type 'unicode'> <type 'unicode'>\n",
      "catch\n",
      "ok\n",
      "3 .. 2 .. 1 .. end\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "import urllib , requests , sys ,string ,time\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "\n",
    "#daito-i\n",
    "#fkey作者來源key.txt\n",
    "#fcomiclist結果key_daito-i.text\n",
    "\n",
    "#輸出格式：\n",
    "#daito-i\n",
    "#!作者\n",
    "#!總筆數\n",
    "#==類別_數量_中文敘述\n",
    "#條目\n",
    "\n",
    "#comiclist常數\n",
    "pnum = 30#頁顯示數量\n",
    "mlink = 'http://www.daito-i.com/top/'#前綴網址\n",
    "\n",
    "fkey = open('key.txt', 'r')\n",
    "key = fkey.readline()#key=作者\n",
    "fkey.close()\n",
    "\n",
    "#檢查BOM\n",
    "if '%EF%BB%BF' in urllib.quote(key):\n",
    "    print 'fuck ms'\n",
    "\n",
    "'''\n",
    "link = \"http://comiclist.jp/index.php?p=s&mode=ss&keyword=\" + urllib.quote(key) + \"&type=title\"\n",
    "res = requests.get(link)\n",
    "'''\n",
    "#當sjis輸出utf8的url\n",
    "key2 = urllib.quote(key.decode('utf8').encode('sjis'))\n",
    "#utf8的url翻sjis\n",
    "key3 = urllib.unquote(key2.decode('sjis').encode('utf8'))\n",
    "#虎穴用sjis\n",
    "\n",
    "#print key2\n",
    "#print key3\n",
    "adict = { \n",
    "'mode':'search',\n",
    "'page_num':'0',\n",
    "'search_cat':'',\n",
    "'keyword':key\n",
    "}\n",
    "res = requests.post(\"http://www.daito-i.com/top/show_unit.php\", data = adict)\n",
    "\n",
    "res.encoding =  res.apparent_encoding#亂碼處理\n",
    "only_a_tags = SoupStrainer(id='contents')#縮小檢索範圍\n",
    "soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)#.prettify()#prettify_縮進顯示html\n",
    "\n",
    "#換頁\n",
    "def next(page = 2):\n",
    "    #http://comiclist.jp/index.php?p=s&mode=ss&type=title&keyword=%E4%B8%8A%E8%97%A4%E6%94%BF%E6%A8%B9&andor=and&maxline=30&pgn=3&pgn=1\n",
    "    #&andor=and&maxline=無影響&pgn=無影響&pgn=頁\n",
    "    #link = \"http://comiclist.jp/index.php?p=s&mode=ss&type=title&keyword=\" + urllib.quote(key) + \"&andor=and&maxline=30&pgn=1&pgn=\" + str(page)\n",
    "    adict = { \n",
    "        'mode':'search',\n",
    "        'category':'',\n",
    "        'subcategory':'',\n",
    "        'search_cat':'',\n",
    "        'keyword':key,\n",
    "        'sort':'',\n",
    "        'page_num':str(page)\n",
    "    }\n",
    "    res = requests.post(\"http://www.daito-i.com/top/show_unit.php\", data = adict)\n",
    "    res.encoding =  res.apparent_encoding\n",
    "    only_a_tags = SoupStrainer(id='contents')\n",
    "    soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)#.prettify()\n",
    "    return soup\n",
    "\n",
    "#全轉半\n",
    "def Q2B(ustring):\n",
    "    fs='０１２３４５６７８９ＱｑＷｗＥｅＲｒＴｔＹｙＵｕＩｉＯｏＰｐＡａＳｓＤｄＦｆＧｇＨｈＪｊＫｋＬｌＺｚＸｘＣｃＶｖＢｂＮｎＭｍ'\n",
    "    hs='0123456789QqWwEeRrTtYyUuIiOoPpAaSsDdFfGgHhJjKkLlZzXxCcVvBbNnMm'\n",
    "    rstr = ''\n",
    "    for tm in ustring:\n",
    "        if fs.find(tm)+1:\n",
    "            tm = hs[fs.find(tm)]\n",
    "        rstr = rstr + tm\n",
    "    ustring = rstr\n",
    "    return ustring\n",
    "#半轉全\n",
    "def B2Q(ustring):\n",
    "    fs='０１２３４５６７８９ＱｑＷｗＥｅＲｒＴｔＹｙＵｕＩｉＯｏＰｐＡａＳｓＤｄＦｆＧｇＨｈＪｊＫｋＬｌＺｚＸｘＣｃＶｖＢｂＮｎＭｍ'\n",
    "    hs='0123456789QqWwEeRrTtYyUuIiOoPpAaSsDdFfGgHhJjKkLlZzXxCcVvBbNnMm'\n",
    "    rstr = ''\n",
    "    for tm in ustring:\n",
    "        if hs.find(tm)+1:\n",
    "            tm = fs[hs.find(tm)]\n",
    "        rstr = rstr + tm\n",
    "    ustring = rstr\n",
    "    return ustring\n",
    "\n",
    "#資料儲存\n",
    "def save(sdict , check=0):\n",
    "    #print listdata\n",
    "    for temp in listdata:\n",
    "        if sdict.get(temp):\n",
    "            #fout.write(temp.encode('utf8') +sdict[temp].encode('utf8')  + '\\n')\n",
    "            fout.write(sdict[temp].encode('utf8')  + '\\n')\n",
    "    #return\n",
    "\n",
    "#新刊\n",
    "def find_1():\n",
    "    #收網址\n",
    "    #網址get\n",
    "    #找出目錄_找出篇名\n",
    "    #回報目錄篇名\n",
    "    print 'find1'\n",
    "    return 'a'\n",
    "#單行本\n",
    "def find_2(blink='http://www.daito-i.com/top/'):\n",
    "    global bdict2\n",
    "    bdate = ''\n",
    "    res = requests.get(blink)\n",
    "    res.encoding =  res.apparent_encoding\n",
    "    only_a_tags = SoupStrainer(class_=\"rec\")\n",
    "    soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)\n",
    "    print soup.select('strong')[0].text,soup.select('strong')[1].text#書名_作者\n",
    "    #print len(soup.select('.goodTxt')),soup.select('.goodTxt')[0]#主內容\n",
    "    \n",
    "    nnum = str(soup.select('.goodTxt')[0]).count('<br/>')#<br/>次數\n",
    "    for tm in soup.select('.goodTxt')[0].select('p'):\n",
    "        if (str(tm).count('<br/>') > 5):# and str(tm).count(key):\n",
    "            #print tm.text,type(tm.text)\n",
    "            bdate = bdate + '(' + str(str(tm).count('<br/>')) + ')-'#單行本話數\n",
    "            bdict2 = bdict2 + tm.get_text('_') + '_'\n",
    "            tm = tm.get_text('\\n-')\n",
    "            bdate = bdate + tm + '%\\n'\n",
    "    #print 'find2'\n",
    "    bdate = bdate[:bdate.find(u'%')+1]#去多餘\n",
    "    return bdate,nnum\n",
    "#雜誌類\n",
    "def find_3(blink='http://www.daito-i.com/top/'):\n",
    "    bdate = ''\n",
    "    res = requests.get(blink)\n",
    "    res.encoding =  res.apparent_encoding\n",
    "    only_a_tags = SoupStrainer(class_=\"rec\")\n",
    "    soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)\n",
    "    #print soup.select('strong')[0].text,soup.select('strong')[1].text#雜誌名_期號\n",
    "    #print len(soup.select('.goodTxt')),soup.select('.goodTxt')[0]#主內容\n",
    "    \n",
    "    nnum = str(soup.select('.goodTxt')[0]).count(KEY)#作者出現次數\n",
    "    for tm in soup.select('.goodTxt')[0].select('p'):\n",
    "        if (str(tm).count('】') > 3) and str(tm).count(KEY):#特徵與作者\n",
    "            #print tm.text,type(tm.text)\n",
    "            tm=tm.text\n",
    "            while tm.find(u'【') > 0:#特徵存在\n",
    "                if KEY.decode('utf8') in tm[tm.find(u'【')+1:tm.find(u'】')]:#符合作者\n",
    "                    listdict3.append(tm[:tm.find(u'【')])\n",
    "                    bdate = bdate + tm[:tm.find(u'】')+1] + '_'#疊加\n",
    "                tm=tm[tm.find(u'】')+1:]\n",
    "    #print 'find3'\n",
    "    return bdate,nnum\n",
    "\n",
    "def find_x(blink='http://www.daito-i.com/top/comics/detail.php?code=9784860329488'):\n",
    "#def find_x(blink='http://www.daito-i.com/top/magazine/detail.php?code=4910137590557'):\n",
    "    #bdate = []\n",
    "    bdate = ''\n",
    "    res = requests.get(blink)\n",
    "    res.encoding =  res.apparent_encoding\n",
    "    only_a_tags = SoupStrainer(class_=\"rec\")\n",
    "    #class=\"rec\"\n",
    "    soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)#.prettify()\n",
    "    #找出目錄_找出篇名\n",
    "    #print len(soup.find_all(text=key.decode('utf8')))\n",
    "    print soup.select('strong')[0].text,soup.select('strong')[1].text\n",
    "    #print len(soup.select('.goodTxt')),soup.select('.goodTxt')[0]\n",
    "    \n",
    "    nnum = str(soup.select('.goodTxt')[0]).count(KEY)#作者出現次數\n",
    "    print 'name_num:',nnum\n",
    "    #print key\n",
    "    #print '\\n',w.count(key)#.decode('utf8'))\n",
    "    #print w.count('<p>'),w.count('<br/>')\n",
    "    #print len(soup.select('.goodTxt')[0].select('p'))\n",
    "    #p = '】'\n",
    "    #ww = ''\n",
    "    print '===='\n",
    "    #'''\n",
    "    for tm in soup.select('.goodTxt')[0].select('p'):\n",
    "        #tm = str(tm)\n",
    "        #print ww.count('】')\n",
    "        if (str(tm).count('】') > 3) and str(tm).count(KEY):\n",
    "            #print tm.count('】')\n",
    "            print tm.text,type(tm.text)\n",
    "            #tm = str(tm)\n",
    "            tm=tm.text\n",
    "            #v = 0\n",
    "            #print tm\n",
    "            while tm.find(u'【') > 0:\n",
    "                #tmm = str(tm)[str(tm).find('【'):str(tm).find('】')]\n",
    "                #print tm\n",
    "                if KEY.decode('utf8') in tm[tm.find(u'【')+1:tm.find(u'】')]:\n",
    "                    print 'ok'#,tm[0:tm.find(u'【')]\n",
    "                    #print len(tm)\n",
    "                    #bdate.append(tm[:tm.find(u'【')])\n",
    "                    bdate = bdate + tm[:tm.find(u'【')] + '_'\n",
    "                    \n",
    "                print tm.find(u'【'),tm.find(u'】')\n",
    "                #print tm[tm.find(u'【'):tm.find(u'】')]\n",
    "                tm=tm[tm.find(u'】')+1:]\n",
    "                #print tm\n",
    "                #print 'oo'\n",
    "    #回報目錄篇名\n",
    "    #'''\n",
    "    print 'find3'\n",
    "    return bdate,nnum\n",
    "\n",
    "#資料處理\n",
    "def findbook(soup , page = 1):\n",
    "    global pnn\n",
    "    a =0\n",
    "    #while len(soup.select('.itmBox')[a]) > 3:\n",
    "    for itmBox in soup.select('.itmBox'):\n",
    "        #itmBox = soup.select('.itmBox')[a]\n",
    "        #print 'aa'+soup.select('.itmBox')[a].text+'bb'\n",
    "        #if soup.select('.itmBox')[a].select('a')[0].get('href'):\n",
    "            #print 'ok'\n",
    "        ctype = soup.select('.marks')[a].next_sibling[1:]\n",
    "        cbook = soup.select('.itmBox')[a].select('a')[1].text\n",
    "        cname = soup.select('.itmBox')[a].select('a')[2].text\n",
    "        \n",
    "        #作品網址\n",
    "        blink = ''\n",
    "        blink = soup.select('.itmBox')[a].select('a')[0].get('href')\n",
    "        blink = mlink + blink\n",
    "        #print blink\n",
    "        \n",
    "        #書名\n",
    "        #print cbook,cname\n",
    "        #print type(itmBox),itmBox\n",
    "        \n",
    "        #類型\n",
    "        #print ctype\n",
    "        bdate = ''#次層頁面主資料\n",
    "        \n",
    "        b = str(pnn).rjust(3,'0')#倒數的流水號\n",
    "        #予約商品_新刊\n",
    "        if ctype in u'予約商品':#予約商品_\n",
    "            #print pnn,ctype,cbook,cname\n",
    "            !bdate=find_1()\n",
    "            book = cname + '_' + cbook + '_\\n!' + blink\n",
    "            dict1.setdefault(pnn,book)\n",
    "        #コミックス_單行本\n",
    "        elif ctype in u'コミックス':\n",
    "            if KEY.decode('utf8') in itmBox.text:\n",
    "                #print pnn,ctype,cbook,cname\n",
    "                cfind=find_2(blink)#目標作者的單行本\n",
    "                print cfind[1]\n",
    "                bdate = cfind[0]\n",
    "                if len(bdate) < 10:\n",
    "                    bdate = 'lost'\n",
    "                book = cbook + '_\\n' + bdate + '\\n!' + blink\n",
    "                dict2.setdefault(pnn,book)\n",
    "            elif cname in u'アンソロジー ':\n",
    "                #print ctype,cbook,cname,'==單>雜'\n",
    "                cfind=find_3(blink)#非作者or合本_アンソロジー \n",
    "                print cfind[1],'num'\n",
    "                bdate=cfind[0]\n",
    "                book = cname + '_' + cbook + '_' + bdate + '_\\n!' + blink\n",
    "                dict2.setdefault(pnn,book)\n",
    "            else:\n",
    "                #print ctype,cbook,cname\n",
    "                book = ctype + '_' + cname + '_' + cbook + '_\\n!' + blink\n",
    "                dict2.setdefault(pnn,book)\n",
    "        #雑誌_雜誌單篇\n",
    "        elif ctype in u'雑誌':\n",
    "            print ctype,cbook,cname\n",
    "            cfind=find_3(blink)\n",
    "            print cfind[1],'name_num'\n",
    "            bdate=cfind[0]\n",
    "            book = cbook + cname + '_' + bdate + '_\\n!' + blink\n",
    "            dict3.setdefault(pnn,book)\n",
    "        #ノベルズ_文庫_画集_書籍_\n",
    "        elif ctype in [u'ノベルズ',u'文庫',u'画集',u'書籍']:\n",
    "            #print ctype,cbook,cname\n",
    "            book = ctype + '_' + cname + '_' + cbook + '_\\n!' + blink\n",
    "            dict4.setdefault(pnn,book)\n",
    "        #ムック_同人誌_販促品_\n",
    "        elif ctype in [u'ムック',u'同人誌',u'販促品']:\n",
    "            #print ctype,cbook,cname\n",
    "            book = ctype + '_' + cname + '_' + cbook + '_\\n!' + blink\n",
    "            dict5.setdefault(pnn,book)\n",
    "        \n",
    "        #\n",
    "        listdata.append(pnn)\n",
    "        '''\n",
    "        1\n",
    "        予約商品_\n",
    "        2\n",
    "        コミックス_\n",
    "        3\n",
    "        雑誌_\n",
    "        4\n",
    "        ノベルズ_文庫_画集_書籍_\n",
    "        5\n",
    "        ムック_同人誌_販促品_\n",
    "        '''\n",
    "        \n",
    "        #print soup.select('.itmBox')[a].br\n",
    "            \n",
    "        #print soup.select('.marks')[2].next_element\n",
    "        #print itmBox.br\n",
    "        \n",
    "            \n",
    "        '''#\n",
    "        cname = soup.select('.list-name')[a].text\n",
    "        cbook = soup.select('strong')[a].text\n",
    "        #print soup.select('strong')[a]\n",
    "        cdata = soup.select('.list-day')[a].text\n",
    "        \n",
    "        #日期處理\n",
    "        cdata = cdata.rstrip()\n",
    "        if len(cdata) < 8:#無日期\n",
    "            cdata = '0000/00/00'#填入日期\n",
    "            check = 4#新作\n",
    "        data = cdata[:10]\n",
    "        while listdata.count(data):#重複日期判斷\n",
    "            data = data[:8] + str(int(data[8:]) + 1).rjust(2,'0')#日期+1_十位數填0\n",
    "        listdata.append(data)\n",
    "        \n",
    "        #字典新增\n",
    "        if check > 0:\n",
    "            if check == 1:\n",
    "                book = book + '_' + name + '\\n' + mlink +blink#書名+作者+網址\n",
    "                dict1.setdefault(data,book)\n",
    "            elif check ==2:\n",
    "                dict2.setdefault(data,book)\n",
    "            elif check ==3:\n",
    "                dict3.setdefault(data,book)\n",
    "            elif check ==4:\n",
    "                dict4.setdefault(data,book)\n",
    "        '''#\n",
    "            \n",
    "        a = a + 1\n",
    "        \n",
    "        #特徵itmBox開滿每頁顯示才放資料，會跑出空值，以搜尋筆數跳出。\n",
    "        pnn = pnn -1\n",
    "        #print a,pnn,int(pnn)\n",
    "        if pnn == 0:\n",
    "            break\n",
    "        \n",
    "    print '========'\n",
    "\n",
    "########\n",
    "KEY = B2Q(key)#.decode('utf8'))\n",
    "#KEY= KEY.encode('utf8')\n",
    "#KEY = KEY.decode('utf8')\n",
    "\n",
    "#if \n",
    "pn =''\n",
    "pn = soup.select('tr')[1].select('td')[1].text\n",
    "#pnn = int(pn)\n",
    "#print pn.find(u'件')\n",
    "pn = pn[0:pn.find(u'件')]#筆數\n",
    "print pn\n",
    "\n",
    "#資料筆數_是否數字\n",
    "if pn > 0:\n",
    "    if int(pn) > pnum:\n",
    "        print 'BIG'\n",
    "    \n",
    "    fout = open(key.decode('utf8') + '_daito-i.txt', 'w')#寫入模式開檔\n",
    "    fout.write('daito-i\\n')#comiclist\n",
    "    print key.decode('utf8') , pn , 'num\\n========'\n",
    "    time.sleep(1)\n",
    "    fout.write('!' + key + '\\n!總筆數' + pn.encode('utf8') + '\\n')\n",
    "    \n",
    "    p = 0#頁\n",
    "    #建空輸出用字典與陣列\n",
    "    #單行本_雜誌單篇_新刊_其它\n",
    "    dict1={}#新刊\n",
    "    dict2={}#單行本\n",
    "    dict3={}#雜誌\n",
    "    dict4={}#\n",
    "    dict5={}#其他\n",
    "    listdict3=[]#雜誌單篇\n",
    "    bdict2 = ''#單行本書目\n",
    "    listdata = []\n",
    "    pnn = int(pn)#計步\n",
    "    \n",
    "    #資料處理\n",
    "    while (int(pn) - p * pnum) > 0:\n",
    "        \n",
    "        print 'page:' + str(p)\n",
    "        soup = next(p)#頁\n",
    "        findbook(soup)#資料處理\n",
    "        p = p + 1\n",
    "        time.sleep(1)\n",
    "    \n",
    "    #日期排序\n",
    "    listdata.sort()\n",
    "    #print '======'\n",
    "    \n",
    "    #sys.exit()################\n",
    "    temp = ''\n",
    "    #dict1_雜誌輸出\n",
    "    fout.write('==new_' + str(len(dict1)) +'_新刊\\n')\n",
    "    save(dict1)\n",
    "    #dict2_單行本輸出\n",
    "    fout.write('==book_' + str(len(dict2)) +'_單行本\\n')\n",
    "    save(dict2)\n",
    "    #dict3_雜誌輸出\n",
    "    fout.write('==adult_' + str(len(dict3)) +'_雜誌\\n')\n",
    "    save(dict3)\n",
    "    #dict4_作畫擔任輸出\n",
    "    fout.write('==art_' + str(len(dict4)) +'_作畫擔任\\n')\n",
    "    save(dict4)\n",
    "    #dict5_其他輸出\n",
    "    fout.write('==other_' + str(len(dict5)) +'_其他\\n')\n",
    "    save(dict5)\n",
    "    \n",
    "    fout.write('==fd32_' + str(len(listdict3)) +'_比對結果\\n')\n",
    "    tmp = ''\n",
    "    out32 = ''\n",
    "    #listdict3_bdict2\n",
    "    for tmp in listdict3:\n",
    "        #print type(tmp),type(bdict2)\n",
    "        if tmp in bdict2:\n",
    "            #print 'catch'\n",
    "            tmp='-'+tmp\n",
    "        out32=out32+tmp+'\\n'\n",
    "    fout.write(out32.encode('utf8'))\n",
    "    \n",
    "    fout.close()\n",
    "    print 'ok'\n",
    "elif p:\n",
    "    print '同人'\n",
    "\n",
    "#結束讀秒\n",
    "x=3\n",
    "while x!=0:\n",
    "    print x,'..',\n",
    "    x=x-1\n",
    "    time.sleep(1)\n",
    "print 'end'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フリージング２８ 金光鉉／原作：林達永&nbsp&nbsp\n",
      "第１９０話　拒絶Ｉ第１９１話　拒絶ＩＩ第１９２話　拒絶ＩＩＩ第１９３話　役割Ｉ第１９４話　役割ＩＩ第１９５話　価値第１９６話　相応の力 <type 'unicode'>\n",
      "<type 'unicode'>\n",
      "(6)-第１９０話　拒絶Ｉ\n",
      "-第１９１話　拒絶ＩＩ\n",
      "-第１９２話　拒絶ＩＩＩ\n",
      "-第１９３話　役割Ｉ\n",
      "-第１９４話　役割ＩＩ\n",
      "-第１９５話　価値\n",
      "-第１９６話　相応の力%\n",
      "= 11\n",
      "ok\n",
      "<type 'str'>\n",
      "<type 'str'> <type 'str'>\n",
      "こりす こりす\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "import urllib , requests , sys ,string ,time\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "\n",
    "fkey = open('key.txt', 'r')\n",
    "key = fkey.readline()#key=作者\n",
    "fkey.close()\n",
    "\n",
    "#檢查BOM\n",
    "if '%EF%BB%BF' in urllib.quote(key):\n",
    "    print 'fuck ms'\n",
    "\n",
    "'''\n",
    "link = \"http://comiclist.jp/index.php?p=s&mode=ss&keyword=\" + urllib.quote(key) + \"&type=title\"\n",
    "res = requests.get(link)\n",
    "'''\n",
    "#當sjis輸出utf8的url\n",
    "key2 = urllib.quote(key.decode('utf8').encode('sjis'))\n",
    "#utf8的url翻sjis\n",
    "key3 = urllib.unquote(key2.decode('sjis').encode('utf8'))\n",
    "#虎穴用sjis\n",
    "\n",
    "#print key2\n",
    "#print key3\n",
    "adict = { \n",
    "'mode':'search',\n",
    "'page_num':'0',\n",
    "'search_cat':'',\n",
    "'keyword':key\n",
    "}\n",
    "'''\n",
    "res = requests.post(\"http://www.daito-i.com/top/show_unit.php\", data = adict)\n",
    "\n",
    "res.encoding =  res.apparent_encoding#亂碼處理\n",
    "only_a_tags = SoupStrainer(id='contents')#縮小檢索範圍 id=\"wrapper\"\n",
    "'''\n",
    "h1='''\n",
    "<li>\n",
    "<div class=\"itmBox\">\t<a href=\"magazine/detail.php?code=4910137600157\">\n",
    "\t<img src=\"images/magazine/max/2014/4910137600157.jpg\" alt=\"ＣＯＭＩＣ漫華鏡[２０１５年１月号]\"></a><br />\n",
    "<div class=\"marks\"></div>\n",
    "雑誌<br />\n",
    "「<a href=\"magazine/detail.php?code=4910137600157\">ＣＯＭＩＣ漫華鏡</a>\n",
    "」<br />\n",
    "<a href=\"show_unit.php?mode=search&disp_type=2&keyword=%EF%BC%92%EF%BC%90%EF%BC%91%EF%BC%95%E5%B9%B4%EF%BC%91%E6%9C%88%E5%8F%B7\">２０１５年１月号</a>&nbsp&nbsp<br />\n",
    "<span class=\"itmPrc\">400円</span>\n",
    "</div></li>\n",
    "'''\n",
    "\n",
    "h2='''\n",
    "<html>\n",
    "<body>\n",
    "<li>\n",
    "<div class=\"itmBox\"><a href=\"detail.php?code=9784894656833\">[詳細ページ]</a>\n",
    "<img src=\"../images/printing.gif\"><br><br />\n",
    "<div class=\"marks\"><img src=\"../img/new.gif\" alt=\"new\" align=\"absmidedle\" border=\"0\">&nbsp;</div>\n",
    "予約商品<br />\n",
    "「<a href=\"detail.php?code=9784894656833\">ガールズ・マジョリティー</a>\n",
    "」<br />\n",
    "<a href=\"../show_unit.php?mode=search&disp_type=2&keyword=%E9%AB%98%E5%B2%A1%E5%9F%BA%E6%96%87\">高岡基文</a>&nbsp&nbsp<br />\n",
    "<span class=\"itmPrc\">1,080円</span>\n",
    "</div></li>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "soup = ''\n",
    "sou = ''\n",
    "#soup = BeautifulSoup(res.text ,\"lxml\" ,  parse_only=only_a_tags)#.prettify()\n",
    "soup = BeautifulSoup(h1 ,\"lxml\")# ,  parse_only=only_a_tags)#.prettify()\n",
    "sou = BeautifulSoup(h2 ,\"lxml\")\n",
    "#print soup.select('.marks')[0].next_sibling[1:]\n",
    "#print sou.select('.marks')[0].next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling\n",
    "#print sou.a\n",
    "#print soup.select('a')[1].text,soup.select('a')[2].text\n",
    "#print sou.select('a')[1].text,sou.select('a')[2].text\n",
    "#\n",
    "#全轉半\n",
    "def Q2B(ustring):\n",
    "    fs='０１２３４５６７８９ＱｑＷｗＥｅＲｒＴｔＹｙＵｕＩｉＯｏＰｐＡａＳｓＤｄＦｆＧｇＨｈＪｊＫｋＬｌＺｚＸｘＣｃＶｖＢｂＮｎＭｍ'\n",
    "    hs='0123456789QqWwEeRrTtYyUuIiOoPpAaSsDdFfGgHhJjKkLlZzXxCcVvBbNnMm'\n",
    "    rstr = ''\n",
    "    for tm in ustring:\n",
    "        if fs.find(tm)+1:\n",
    "            tm = hs[fs.find(tm)]\n",
    "        rstr = rstr + tm\n",
    "    ustring = rstr\n",
    "    return ustring\n",
    "#半轉全\n",
    "def B2Q(ustring):\n",
    "    fs='０１２３４５６７８９ＱｑＷｗＥｅＲｒＴｔＹｙＵｕＩｉＯｏＰｐＡａＳｓＤｄＦｆＧｇＨｈＪｊＫｋＬｌＺｚＸｘＣｃＶｖＢｂＮｎＭｍ'\n",
    "    hs='0123456789QqWwEeRrTtYyUuIiOoPpAaSsDdFfGgHhJjKkLlZzXxCcVvBbNnMm'\n",
    "    rstr = ''\n",
    "    for tm in ustring:\n",
    "        if hs.find(tm)+1:\n",
    "            tm = fs[hs.find(tm)]\n",
    "        rstr = rstr + tm\n",
    "    ustring = rstr\n",
    "    return ustring\n",
    "\n",
    "def find_x(blink='http://www.daito-i.com/top/comics/detail.php?code=9784799207833'):\n",
    "    bdate = ''\n",
    "    res = requests.get(blink)\n",
    "    res.encoding =  res.apparent_encoding\n",
    "    only_a_tags = SoupStrainer(class_=\"rec\")\n",
    "    soup = BeautifulSoup(res.text ,\"lxml\",  parse_only=only_a_tags)\n",
    "    print soup.select('strong')[0].text,soup.select('strong')[1].text#書名_作者\n",
    "    #print len(soup.select('.goodTxt')),soup.select('.goodTxt')[0]#主內容\n",
    "    \n",
    "    nnum = str(soup.select('.goodTxt')[0]).count('<br/>')#換行出現次數\n",
    "    for tm in soup.select('.goodTxt')[0].select('p'):\n",
    "        #print tm.prettify()\n",
    "        if (str(tm).count('<br/>') > 5):# and str(tm).count(key):\n",
    "            print tm.text,type(tm.text)\n",
    "            bdate = bdate + '(' + str(str(tm).count('<br/>')) + ')-'\n",
    "            #tm=str(tm).replace('<br/>', '\\n')\n",
    "            tm = tm.get_text('\\n-')\n",
    "            print type(tm)\n",
    "            bdate = bdate + tm + '%\\n'\n",
    "            '''\n",
    "            while tm.find(u'【') > 0:\n",
    "                if key.decode('utf8') in tm[tm.find(u'【')+1:tm.find(u'】')]:\n",
    "                    bdate = bdate + tm[:tm.find(u'【')] + '_'\n",
    "                tm=tm[tm.find(u'】')+1:]\n",
    "            #'''\n",
    "    #print 'find3'\n",
    "    bdate = bdate[:bdate.find(u'%')+1]#去多餘\n",
    "    return bdate,nnum\n",
    "#print len(key.decode('utf8')) ,len(str(key)), find_3(),str(key)\n",
    "cb,bb=find_x()\n",
    "#print type(cb)\n",
    "print cb\n",
    "print '=',bb\n",
    "\n",
    "if 'sa' in 'gfdsa':\n",
    "    print  'ok'\n",
    "print type(key)\n",
    "KEY = B2Q(key)\n",
    "KEY= KEY\n",
    "print type(key), type(KEY)\n",
    "print key,KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
